\paragraph 熵估计主要理论

考虑到转换器的理论意义只是“提取出相似信息的能力”，为了进一步衡量隐层表征的相似度，需要引入新的指标。不妨先考虑$A_l$与$B_r$（$l< r$）的相似度，转换器衡量的是从$A_l$提取出接近$B_r$的信息的能力，那么如果有一种方法能衡量$A_l$与$B_r$的信息量或信息复杂程度，那么就能基于转换器提取出的信息占双方信息量的比例而对两个隐层的相似度做估计。事实上其余情况也可以类似地处理，这将在下一节具体叙述。

衡量信息量有一个很传统的方法，即考虑隐层输出的熵，具体地说，是假定隐层输出是某个欧式空间中的连续型随机变量，并估算其微分熵。事实上目前尚没有熵的无偏估计被提出【】，但是渐进意义下，足够好的无偏估计是存在的。【】证明了

Theorem


对于k近邻中k的选取，文献【】中做了一些讨论和实验，部分结果如下

figure

我们经过以高维高斯分布为基础的实验和测算，最终决定取最近邻、次近邻、3-近邻的熵估计的均值作为结果。


如此一来，为估算熵只需计算一系列样本下隐层输出的距离矩阵，该矩阵可用如下方法计算

Lemma

Proof 

该算法的总复杂度为$O(S^2 N_{total})$，S为用于估算的样本个数，$N_{total}$为各个隐层神经元总数。

事实上近邻估计法的收敛速度很慢，但是由于需要使用的只有熵的比，在这种情况下数百的样本已经足够有较好的效果。而CKA的单次复杂度为$O(S N^2)$，其中N为隐层神经元的个数，N这部分的复杂度是难以优化的。更重要的是，计算$O(H^2)$对隐层的相似度时，每次都需重新计算CKA，相比之下熵是可以直接调用的。这种优势在跨模型估算表征相似度的时候尤为明显，当然我们的算法的最终运行时间还取决于转换器的效率。

\paragraph 算法和技术细节

值得一提的是由于以上算法中只用到了向量间的距离，我们还可以使用降维来适度地降低复杂度而不损失过多的估计效果，这一点可以由JL Lemma保证

Lemma

在目前的模型和样本规模上，降维并没有明显的提速效果，但由于近邻估算法对于维数很敏感，一定程度的降维在实验中提高了熵估算的准确性。对于CKA实际上也可以进行类似的降维来提高效率，但这种降维对于CKA的效果的影响是没有理论保证的，但我们并未实际验证过。

此外，微分熵实际上并不是一个很好的信息测度，当向量过于密集的时候它可能为负数，在训练完成的模型中这种现象并未出现，但对于训练前的模型，有一些隐层的熵为负数，代码实现中直接将所有熵加上了1-最小值。更大的缺陷是微分熵对于数乘非常敏感，这与我们的目标相矛盾（在下一节中我们将详述为何要保证相似度的度量不受数乘和正交变换影响）。考虑到高维标准高斯分布的微分熵是常数，且$d$维标准高斯向量的L2-norm的期望为$\sqrt{d}$，算法中使用了将所以向量等比缩放，使最大模长等于$\sqrt{d}$的归一化方式（而不是使均值等于$\sqrt{d}$，主要是基于对高斯分布进行熵估算测试的结果所做的调整），而在最后一层linear层并不做类似的调整（原因也见下一节）。

至此，完整的熵估算算法如下

algorithm






相似度估算方法

主要理论

事实上在后面的实验中我们可以看到转换器和熵的实验结果已经可以表明我们的网络确实学到了一些表征方式，而且具有不同初始化权重的网络最终习得的表征方式在很多方面是相似的。为了得到一个和CKA类似的能层对层衡量相似度的方法，并得到能胜任相同应用的算法，我们将把转换器与熵结合起来。

对于$l$不等于$r$的情形（不妨设$l<r$），转换器从$A_l$提取出来与$B_r$进行比较的信息表现了两者共有的信息，我们以转换器的loss除以$B_r$的l2-norm，得$convert_loss(l,r)$，以$convert_ratio(l,r)=1-convert_loss(l,r)$来估计共有信息在$B_r$总信息中的占比，用共有信息在$A_l$总信息中的占比作为相似度指标，这过程中的转化通过乘$H_B(r)/H_A(l)$实现。

对于$l$和$r$相等的情形，若两个隐层都是最后一个linear层，可以用两个模型在训练集和测试集上的预测相同的比例乘上两者的熵之比来估计相似度。否则我们取$B_{r+t}(t>0)$，假设$A_l$与$B_r$的共有信息占$B_r$的比例与$convert_ratio(l,r+t)$接近，再利用类似的方法计算相似度。

这部分虽然具有可解释性，但并没有足够坚实的理论基础，目前实验结果能起到一定的支持作用，如果能加入residual connection再进行实验并得出具有栅格（详见CKA【】）形式的结果，或许能进一步说明这种方法的有效性。

算法和技术细节

在实际实验中发现，如果直接任取一些样本估算熵，不同label的数据会在隐层间逐层被区别开，而同一label的数据则不断被聚集，熵的变化反而不明显，最终采用的方案是将不同label下的隐层输出分开估计熵，计算熵比时将各个label下的熵比取平均。

而在相似度计算方面，对于$l$和$r$相同的情况，进行了t的枚举和取平均，以及对称情况的取max。最终的算法中，隐层$A_l$、$B_r$的相似度指标$Si_{l,r}$如下：




可以注意到，如果我们认为$A_l$的信息远多于$B_r$且包含了后者，那么在B的视角，对$B_r$而言$A_l$与它相似，我们也可以把上述的相似度指标拆解为非对称的形式。

在最初的思考与讨论中，我们认为用于估算相似度的指标应当具备在可逆线性变换下的不变性，在卷积操作下也应当有一定程度的不变性，因为如果我们对某个隐层作用某种程度上可逆的卷积操作或可逆线性变换，可以通过调整下一层的网络权重而提取出相同的信息。倘若我们的指标在相似度结果合理的同时，也具备类似的性质，将是非常理想的。但是另一方面，这种“可逆”性过于宽泛，而“调整下一层网络“实际上并不会发生，限制得太紧难以得到有效的相似度指标，故最终与【】相似地，我们设计了在数乘和正交变换下不变的相似度指标。


结论和未来可能的工作

我们的工作至少能从三个角度不同程度地说明了具备不同初始化权重的网络习得了相似的表征，一是对于完成训练的两个模型，转换器在1个epoch下就可以有极强的效果，20个epoch时甚至可以达到个位数的loss，而对于未训练的两个模型，尽管转换器在1个epoch下的表现劣化不明显，却难以通过继续训练而降低loss或提高acc，当在训练完成的模型和未训练模型之间转换时，转换器一直保持了很差的效果；二是模型在训练完成前后熵的变化较大，而且两个网络隐层逐层的熵变化规律有相似之处；三是最终得出的相似度也表面了两个网络的隐层间有着较强的相似关系，对于两个未训练的网络进行对比、未训练网络和已训练网络进行对比的情况则不然。

同时我们现有的方法可以处理【】提出的CKA所能处理的应用，同时具备比CKA更好的时空效率，相似度指标的绝对数值也更合理，而且有一定的理论基础和可解释性。

之后我们将使用更大规模的数据集、更大规模的模型进行实验，并尝试使用不同于微分熵的、更合理有效的信息测度，也可能会考虑用与文献【】类似的方式，引入带反卷积层的转换器参与$l=r$情况的处理。同时我们也进行过一系列对抗测试，目前还没有非常明朗的结果，之后也将会推进。


